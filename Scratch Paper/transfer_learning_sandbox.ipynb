{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations & Configuration"
      ],
      "metadata": {
        "id": "jNgoPgsqdUbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "kaggle_json_path = '/content/drive/MyDrive/ColabNotebooks/A5/kaggle.json'\n",
        "\n",
        "# Copy kaggle.json to the correct location\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp {kaggle_json_path} ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ko-nsGdUM2",
        "outputId": "27ca7a43-a325-4d7c-eb21-f244b450f856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Setup"
      ],
      "metadata": {
        "id": "hUzTJsWIdMeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the movie posters from kaggle\n",
        "!kaggle datasets download -d rezaunderfit/48k-imdb-movies-with-posters > /dev/null 2>&1\n",
        "!unzip -q 48k-imdb-movies-with-posters.zip\n",
        "\n",
        "# Load title basics\n",
        "tsv_path = '/content/drive/MyDrive/ColabNotebooks/A5/title.basics.tsv'\n",
        "title_basics = pd.read_csv(tsv_path, sep='\\t', na_values='\\\\N')\n",
        "\n",
        "# List all files in the Poster directory\n",
        "poster_dir = 'Poster'\n",
        "poster_files = []\n",
        "for root, _, files in os.walk(poster_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.jpg'):\n",
        "            poster_files.append(os.path.join(root, file))\n",
        "\n",
        "# Extract tconst and startYear from file paths\n",
        "poster_info = []\n",
        "for file_path in poster_files:\n",
        "    parts = file_path.split('/')\n",
        "    start_year = parts[1]\n",
        "    tconst = parts[2]\n",
        "    poster_info.append((start_year, tconst))\n",
        "\n",
        "# Convert to DataFrame\n",
        "poster_df = pd.DataFrame(poster_info, columns=['startYear', 'tconst'])\n",
        "\n",
        "# Ensure startYear is an integer\n",
        "poster_df['startYear'] = poster_df['startYear'].astype(int)\n",
        "title_basics['startYear'] = title_basics['startYear'].astype(float).fillna(0).astype(int)  # Handle missing startYear and convert to int\n",
        "\n",
        "# Merge with title_basics to keep only relevant records\n",
        "title_basics_filtered = pd.merge(title_basics, poster_df, on=['startYear', 'tconst'])\n",
        "\n",
        "# Create your data splits\n",
        "train_metadata, test_metadata = train_test_split(title_basics_filtered, test_size=0.2, random_state=42)\n",
        "train_metadata, val_metadata = train_test_split(train_metadata, test_size=0.25, random_state=42)\n",
        "print(f\"Train size: {len(train_metadata)}, Validation size: {len(val_metadata)}, Test size: {len(test_metadata)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Define the image transformations\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize(299),\n",
        "    transforms.CenterCrop(299),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, metadata, img_dir, transform=None, genre_to_index=None):\n",
        "        self.metadata = metadata\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.genre_to_index = genre_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tconst = self.metadata.iloc[idx]['tconst']\n",
        "        start_year = self.metadata.iloc[idx]['startYear']\n",
        "        img_name = os.path.join(self.img_dir, str(start_year), tconst, f\"{tconst}.jpg\")\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        genres = self.metadata.iloc[idx]['genres']\n",
        "        genre_tensor = self.genres_to_tensor(genres)\n",
        "        return image, genre_tensor\n",
        "\n",
        "    def genres_to_tensor(self, genres):\n",
        "        first_genre = genres.split(',')[0] if pd.notna(genres) else 'Unknown'\n",
        "        genre_index = self.genre_to_index.get(first_genre, self.genre_to_index['Unknown'])\n",
        "        return torch.tensor(genre_index, dtype=torch.long)\n",
        "\n",
        "# Create a mapping from genre to index\n",
        "all_genres = set(g.split(',')[0] for g in title_basics_filtered['genres'].dropna())\n",
        "genre_to_index = {genre: idx for idx, genre in enumerate(all_genres)}\n",
        "genre_to_index['Unknown'] = len(genre_to_index)\n",
        "\n",
        "# Directory containing images\n",
        "img_dir = 'Poster'\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MovieDataset(metadata=train_metadata, img_dir=img_dir, transform=image_transforms, genre_to_index=genre_to_index)\n",
        "val_dataset = MovieDataset(metadata=val_metadata, img_dir=img_dir, transform=image_transforms, genre_to_index=genre_to_index)\n",
        "test_dataset = MovieDataset(metadata=test_metadata, img_dir=img_dir, transform=image_transforms, genre_to_index=genre_to_index)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Example of using the DataLoader\n",
        "for images, genres in data_loader:\n",
        "    print(images.shape)  # Shape: (batch_size, 3, 182, 268)\n",
        "    print(genres.shape)  # Shape: (batch_size,)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un3zXmMXdLi0",
        "outputId": "d6d2483c-d0a3-4a01-ff0f-6b7e46c4e657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-c0251d199d45>:7: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  title_basics = pd.read_csv(tsv_path, sep='\\t', na_values='\\\\N')\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 182, 268])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning --> Fine Tuning (Training)"
      ],
      "metadata": {
        "id": "Gx4aXRhgaA4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU3JxRHSZ-Rc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "274a0193-c14f-451f-b76d-c8b417650d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40205, 9)\n",
            "      tconst titleType                                 primaryTitle  \\\n",
            "0  tt0000001     short                                   Carmencita   \n",
            "1  tt0000003     short                               Pauvre Pierrot   \n",
            "2  tt0000005     short                             Blacksmith Scene   \n",
            "3  tt0000007     short  Corbett and Courtney Before the Kinetograph   \n",
            "4  tt0000008     short       Edison Kinetoscopic Record of a Sneeze   \n",
            "\n",
            "                                 originalTitle  isAdult  startYear  endYear  \\\n",
            "0                                   Carmencita      0.0       1894      NaN   \n",
            "1                               Pauvre Pierrot      0.0       1892      NaN   \n",
            "2                             Blacksmith Scene      0.0       1893      NaN   \n",
            "3  Corbett and Courtney Before the Kinetograph      0.0       1894      NaN   \n",
            "4       Edison Kinetoscopic Record of a Sneeze      0.0       1894      NaN   \n",
            "\n",
            "  runtimeMinutes                    genres  \n",
            "0            1.0         Documentary,Short  \n",
            "1            5.0  Animation,Comedy,Romance  \n",
            "2            1.0              Comedy,Short  \n",
            "3            1.0               Short,Sport  \n",
            "4            1.0         Documentary,Short  \n",
            "Total number of unique genres: 26\n"
          ]
        }
      ],
      "source": [
        "# Set up model for fine tuning\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
        "model.aux_logits = False  # Disable auxiliary logits\n",
        "num_genres = len(genre_to_index)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_genres)  # Adjust the final layer\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with validation\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, genres in train_loader:\n",
        "        images = images.to(device)\n",
        "        genres = genres.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, genres)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_images, val_genres in val_loader:\n",
        "            val_images = val_images.to(device)\n",
        "            val_genres = val_genres.to(device)\n",
        "            val_outputs = model(val_images)\n",
        "            val_loss += criterion(val_outputs, val_genres).item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss}\")\n",
        "\n",
        "    # Save the model if validation loss decreases\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning --> Fine Tuning (Testing)"
      ],
      "metadata": {
        "id": "OFkRWZLQYbb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for test_images, test_genres in test_loader:\n",
        "        test_images = test_images.to(device)\n",
        "        test_genres = test_genres.to(device)\n",
        "        test_outputs = model(test_images)\n",
        "        test_loss += criterion(test_outputs, test_genres).item()\n",
        "        _, predicted = torch.max(test_outputs, 1)\n",
        "        total += test_genres.size(0)\n",
        "        correct += (predicted == test_genres).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "test_accuracy = correct / total\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "iHmG-8PGYdNa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}